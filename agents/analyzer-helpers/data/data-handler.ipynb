{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f0d7d1",
   "metadata": {},
   "source": [
    "# Incident Report Deduplication System\n",
    "\n",
    "This notebook processes the `Case Log.xlsx` file and creates a deduplicated CSV with consolidated incident reports and their root causes.\n",
    "\n",
    "## Approach:\n",
    "1. Load the source Excel file\n",
    "2. Clean and preprocess text (remove numbers, IDs, timestamps)\n",
    "3. Use spaCy NLP for phrase-level similarity analysis\n",
    "4. Compare incidents and solutions using 90% similarity threshold\n",
    "5. Generate concise incident reports\n",
    "6. Save to `processed-data/consolidated_incidents.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589d6d0",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db913afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n",
      "spaCy model: core_web_md\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load spaCy model (using medium model for better accuracy)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_md\"])\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"spaCy model: {nlp.meta['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82cf06",
   "metadata": {},
   "source": [
    "## Step 2: Load the Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40fc304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 323 rows from source-data\\Case Log.xlsx\n",
      "\n",
      "Columns in the file: ['Module', 'Mode', 'EDI?', 'TIMESTAMP', 'Alert / Email', 'Problem Statements', 'Solution', 'SOP']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Module</th>\n",
       "      <th>Mode</th>\n",
       "      <th>EDI?</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>Alert / Email</th>\n",
       "      <th>Problem Statements</th>\n",
       "      <th>Solution</th>\n",
       "      <th>SOP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EDI/API</td>\n",
       "      <td>Call</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-02-25 07:34:00</td>\n",
       "      <td>Call ALR-360601 | EDI/API Data Mismatch on HLC...</td>\n",
       "      <td>Time zone drift caused eventTime to serialize ...</td>\n",
       "      <td>Normalized eventTime to port timezone and adde...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EDI/API</td>\n",
       "      <td>SMS</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-06-30 06:08:00</td>\n",
       "      <td>Alert: SMS TCK-265455 | Issue: Spike in DLQ me...</td>\n",
       "      <td>Spike in DLQ messages after routine maintenanc...</td>\n",
       "      <td>Corrected cron schedule to UTC; added mutual e...</td>\n",
       "      <td>EDI: Spike in DLQ messages after routine maint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vessel</td>\n",
       "      <td>SMS</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-07-04 06:29:00</td>\n",
       "      <td>Alert: SMS TCK-936729 | Issue: ANSI X12 301 in...</td>\n",
       "      <td>ANSI X12 301 inconsistency for MV SILVER CURRE...</td>\n",
       "      <td>Enabled conflict resolution preferring max(eve...</td>\n",
       "      <td>VSL: Duplicate EDI De-duplication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EDI/API</td>\n",
       "      <td>Email</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-06-04 00:02:00</td>\n",
       "      <td>Subject: Email ALR-535708 | EDI Parsing Issue ...</td>\n",
       "      <td>EDIFACT CODECO duplicate detected with conflic...</td>\n",
       "      <td>Added de-duplication keyed by (controlNumber, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Container Report</td>\n",
       "      <td>SMS</td>\n",
       "      <td>No</td>\n",
       "      <td>2025-01-15 20:18:00</td>\n",
       "      <td>Alert: SMS TCK-142185 | Issue: Customs flag to...</td>\n",
       "      <td>Customs flag toggled incorrectly for EMCU16695...</td>\n",
       "      <td>Manual verification per CNTR-07 (Yard Location...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Module   Mode EDI?            TIMESTAMP  \\\n",
       "0           EDI/API   Call  Yes  2025-02-25 07:34:00   \n",
       "1           EDI/API    SMS  Yes  2025-06-30 06:08:00   \n",
       "2            Vessel    SMS  Yes  2025-07-04 06:29:00   \n",
       "3           EDI/API  Email  Yes  2025-06-04 00:02:00   \n",
       "4  Container Report    SMS   No  2025-01-15 20:18:00   \n",
       "\n",
       "                                       Alert / Email  \\\n",
       "0  Call ALR-360601 | EDI/API Data Mismatch on HLC...   \n",
       "1  Alert: SMS TCK-265455 | Issue: Spike in DLQ me...   \n",
       "2  Alert: SMS TCK-936729 | Issue: ANSI X12 301 in...   \n",
       "3  Subject: Email ALR-535708 | EDI Parsing Issue ...   \n",
       "4  Alert: SMS TCK-142185 | Issue: Customs flag to...   \n",
       "\n",
       "                                  Problem Statements  \\\n",
       "0  Time zone drift caused eventTime to serialize ...   \n",
       "1  Spike in DLQ messages after routine maintenanc...   \n",
       "2  ANSI X12 301 inconsistency for MV SILVER CURRE...   \n",
       "3  EDIFACT CODECO duplicate detected with conflic...   \n",
       "4  Customs flag toggled incorrectly for EMCU16695...   \n",
       "\n",
       "                                            Solution  \\\n",
       "0  Normalized eventTime to port timezone and adde...   \n",
       "1  Corrected cron schedule to UTC; added mutual e...   \n",
       "2  Enabled conflict resolution preferring max(eve...   \n",
       "3  Added de-duplication keyed by (controlNumber, ...   \n",
       "4  Manual verification per CNTR-07 (Yard Location...   \n",
       "\n",
       "                                                 SOP  \n",
       "0                                                NaN  \n",
       "1  EDI: Spike in DLQ messages after routine maint...  \n",
       "2                  VSL: Duplicate EDI De-duplication  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define paths\n",
    "source_file = Path(\"source-data/Case Log.xlsx\")\n",
    "output_file = Path(\"processed-data/consolidated_incidents.csv\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_file.parent.mkdir(exist_ok=True)\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(source_file)\n",
    "\n",
    "print(f\"Loaded {len(df)} rows from {source_file}\")\n",
    "print(f\"\\nColumns in the file: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ae8de5",
   "metadata": {},
   "source": [
    "## Step 3: Text Preprocessing Functions\n",
    "\n",
    "Create functions to clean and normalize text by removing numbers, IDs, timestamps, and other variable details while preserving the core problem description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a161211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing + caching functions created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Caches to avoid re-processing\n",
    "_doc_cache = {}\n",
    "_phrase_cache = {}\n",
    "_sim_cache = {}\n",
    "\n",
    "\n",
    "def _get_doc(text: str):\n",
    "    if text in _doc_cache:\n",
    "        return _doc_cache[text]\n",
    "    d = nlp(text)\n",
    "    _doc_cache[text] = d\n",
    "    return d\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing numbers, IDs, timestamps, and other variable details\n",
    "    while preserving the core problem description.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove common patterns that vary between incidents\n",
    "    # Remove timestamps (various formats)\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '[DATE]', text)  # 2025-10-18\n",
    "    text = re.sub(r'\\b\\d{1,2}[:/\\-]\\d{1,2}[:/\\-]\\d{2,4}\\b', '[DATE]', text)  # 10/18/2025, 18-10-25\n",
    "    text = re.sub(r'\\b\\d{1,2}:\\d{2}(:\\d{2})?(\\s?[AaPp][Mm])?\\b', '[TIME]', text)  # 10:30, 10:30 AM\n",
    "    \n",
    "    # Remove IDs and numbers (but keep words)\n",
    "    text = re.sub(r'\\b[A-Z]{2,}\\d+\\b', '[ID]', text)  # INC123, REQ456, etc.\n",
    "    text = re.sub(r'\\b\\d{5,}\\b', '[ID]', text)  # Long numbers (IDs)\n",
    "    text = re.sub(r'#\\d+', '[ID]', text)  # Ticket numbers like #12345\n",
    "    \n",
    "    # Remove standalone numbers but keep those within words\n",
    "    text = re.sub(r'\\s\\d+\\s', ' ', text)\n",
    "    text = re.sub(r'^\\d+\\s', '', text)\n",
    "    text = re.sub(r'\\s\\d+$', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def get_phrases(text):\n",
    "    \"\"\"\n",
    "    Split text into meaningful phrases using spaCy's sentence detection.\n",
    "    Cached by cleaned text to avoid recomputation.\n",
    "    \"\"\"\n",
    "    if not text or text == \"\":\n",
    "        return []\n",
    "    \n",
    "    if text in _phrase_cache:\n",
    "        return _phrase_cache[text]\n",
    "    \n",
    "    doc = _get_doc(text)\n",
    "    phrases = []\n",
    "    \n",
    "    # Get sentences as phrases\n",
    "    for sent in doc.sents:\n",
    "        phrase = sent.text.strip()\n",
    "        if len(phrase.split()) >= 2:  # Only keep phrases with 2+ words\n",
    "            phrases.append(phrase)\n",
    "    \n",
    "    _phrase_cache[text] = phrases\n",
    "    return phrases\n",
    "\n",
    "\n",
    "def calculate_phrase_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Calculate similarity between two texts using phrase-level analysis.\n",
    "    Returns the average similarity score across all phrase pairs.\n",
    "    Uses caching to avoid re-computation for the same pair.\n",
    "    \"\"\"\n",
    "    # Clean both texts\n",
    "    clean1 = clean_text(text1)\n",
    "    clean2 = clean_text(text2)\n",
    "    \n",
    "    if not clean1 or not clean2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Use a canonical key for pairwise cache (order-independent)\n",
    "    pair_key = tuple(sorted([clean1, clean2]))\n",
    "    if pair_key in _sim_cache:\n",
    "        return _sim_cache[pair_key]\n",
    "    \n",
    "    # Get phrases from both texts (from cache if available)\n",
    "    phrases1 = get_phrases(clean1)\n",
    "    phrases2 = get_phrases(clean2)\n",
    "    \n",
    "    if not phrases1 or not phrases2:\n",
    "        # Fallback to direct document similarity if no phrases found\n",
    "        doc1 = _get_doc(clean1)\n",
    "        doc2 = _get_doc(clean2)\n",
    "        sim = doc1.similarity(doc2)\n",
    "        _sim_cache[pair_key] = sim\n",
    "        return sim\n",
    "    \n",
    "    # Calculate similarity for all phrase pairs\n",
    "    similarities = []\n",
    "    for p1 in phrases1:\n",
    "        phrase_sims = []\n",
    "        doc1 = _get_doc(p1)\n",
    "        for p2 in phrases2:\n",
    "            doc2 = _get_doc(p2)\n",
    "            sim = doc1.similarity(doc2)\n",
    "            phrase_sims.append(sim)\n",
    "        # For each phrase in text1, take the max similarity with any phrase in text2\n",
    "        if phrase_sims:\n",
    "            similarities.append(max(phrase_sims))\n",
    "    \n",
    "    sim_avg = np.mean(similarities) if similarities else 0.0\n",
    "    _sim_cache[pair_key] = sim_avg\n",
    "    return sim_avg\n",
    "\n",
    "\n",
    "def generate_concise_text(text):\n",
    "    \"\"\"\n",
    "    Generate a concise version of the text by extracting key information.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    \n",
    "    # Remove excessive details and keep the core message\n",
    "    doc = _get_doc(text)\n",
    "    \n",
    "    # Extract key sentences (up to 2 sentences)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    \n",
    "    if len(sentences) <= 2:\n",
    "        return text\n",
    "    \n",
    "    # Keep first two sentences as a concise summary\n",
    "    concise = sentences[0]\n",
    "    if len(sentences) > 1:\n",
    "        concise += \" \" + sentences[1]\n",
    "    \n",
    "    return concise.strip()\n",
    "\n",
    "\n",
    "print(\"Text preprocessing + caching functions created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b503e12",
   "metadata": {},
   "source": [
    "## Step 4: Deduplication Algorithm\n",
    "\n",
    "Implement the core deduplication logic that:\n",
    "1. Iterates through each incident in the source data\n",
    "2. Compares with existing entries in the consolidated CSV\n",
    "3. Uses 90% similarity threshold for both incidents and solutions\n",
    "4. Generates concise versions of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b81d24d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication function created successfully!\n"
     ]
    }
   ],
   "source": [
    "def deduplicate_incidents(source_df, alert_col='Alert/Email', problem_col='Problem Statement', similarity_threshold=0.90):\n",
    "    \"\"\"\n",
    "    Main deduplication function that processes incidents one by one.\n",
    "    \n",
    "    Parameters:\n",
    "    - source_df: Source DataFrame with incident data\n",
    "    - alert_col: Column name for incident alerts/emails\n",
    "    - problem_col: Column name for problem statements\n",
    "    - similarity_threshold: Threshold for considering incidents as duplicates (default 0.90)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with deduplicated incidents\n",
    "    \"\"\"\n",
    "    # Initialize empty list to store consolidated incidents\n",
    "    consolidated = []\n",
    "    \n",
    "    # Track statistics\n",
    "    total_processed = 0\n",
    "    duplicates_found = 0\n",
    "    unique_incidents = 0\n",
    "    \n",
    "    print(f\"Starting deduplication process...\")\n",
    "    print(f\"Similarity threshold: {similarity_threshold * 100}%\\n\")\n",
    "    \n",
    "    # Iterate through each row in the source data\n",
    "    for idx, row in source_df.iterrows():\n",
    "        total_processed += 1\n",
    "        \n",
    "        # Get current incident and problem statement\n",
    "        current_alert = row[alert_col]\n",
    "        current_problem = row[problem_col]\n",
    "        \n",
    "        # Skip rows with empty/null values\n",
    "        if pd.isna(current_alert) or pd.isna(current_problem) or \\\n",
    "           str(current_alert).strip() == \"\" or str(current_problem).strip() == \"\":\n",
    "            continue\n",
    "        \n",
    "        # Generate concise versions\n",
    "        concise_alert = generate_concise_text(current_alert)\n",
    "        concise_problem = generate_concise_text(current_problem)\n",
    "        \n",
    "        # Check if this is a duplicate\n",
    "        is_duplicate = False\n",
    "        \n",
    "        for existing in consolidated:\n",
    "            # Compare incidents using phrase-level similarity\n",
    "            incident_similarity = calculate_phrase_similarity(current_alert, existing['original_alert'])\n",
    "            \n",
    "            if incident_similarity >= similarity_threshold:\n",
    "                # Found a similar incident, now check if the solution is also similar\n",
    "                solution_similarity = calculate_phrase_similarity(current_problem, existing['original_problem'])\n",
    "                \n",
    "                if solution_similarity >= similarity_threshold:\n",
    "                    # Both incident and solution are similar - it's a duplicate\n",
    "                    is_duplicate = True\n",
    "                    duplicates_found += 1\n",
    "                    if total_processed % 10 == 0:\n",
    "                        print(f\"Row {total_processed}: Duplicate found (Incident: {incident_similarity:.2%}, Solution: {solution_similarity:.2%})\")\n",
    "                    break\n",
    "                else:\n",
    "                    # Incident is similar but solution is different - treat as separate incident\n",
    "                    # Continue checking other entries\n",
    "                    continue\n",
    "        \n",
    "        if not is_duplicate:\n",
    "            # Add as new unique incident\n",
    "            consolidated.append({\n",
    "                'Incident_Report': concise_alert,\n",
    "                'Root_Cause': concise_problem,\n",
    "                'original_alert': current_alert,  # Keep for comparison\n",
    "                'original_problem': current_problem  # Keep for comparison\n",
    "            })\n",
    "            unique_incidents += 1\n",
    "            if total_processed % 10 == 0:\n",
    "                print(f\"Row {total_processed}: New unique incident added\")\n",
    "    \n",
    "    # Create final DataFrame (without the original columns used for comparison)\n",
    "    result_df = pd.DataFrame([\n",
    "        {'Incident_Report': item['Incident_Report'], 'Root_Cause': item['Root_Cause']}\n",
    "        for item in consolidated\n",
    "    ])\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DEDUPLICATION COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total rows processed: {total_processed}\")\n",
    "    print(f\"Duplicates found: {duplicates_found}\")\n",
    "    print(f\"Unique incidents: {unique_incidents}\")\n",
    "    print(f\"Reduction: {(duplicates_found/total_processed*100) if total_processed > 0 else 0:.1f}%\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "print(\"Deduplication function created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab3eb28",
   "metadata": {},
   "source": [
    "## Step 5: Run the Deduplication Process\n",
    "\n",
    "Execute the deduplication algorithm on the source data and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84d27ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting deduplication process...\n",
      "Similarity threshold: 90.0%\n",
      "\n",
      "Row 10: New unique incident added\n",
      "Row 20: New unique incident added\n",
      "Row 30: New unique incident added\n",
      "Row 40: Duplicate found (Incident: 97.17%, Solution: 96.54%)\n",
      "Row 50: Duplicate found (Incident: 99.29%, Solution: 99.38%)\n",
      "Row 60: Duplicate found (Incident: 92.22%, Solution: 100.00%)\n",
      "Row 70: New unique incident added\n",
      "Row 80: Duplicate found (Incident: 100.00%, Solution: 98.23%)\n",
      "Row 90: Duplicate found (Incident: 99.14%, Solution: 98.35%)\n",
      "Row 100: New unique incident added\n",
      "Row 110: Duplicate found (Incident: 99.53%, Solution: 98.87%)\n",
      "Row 120: New unique incident added\n",
      "Row 130: Duplicate found (Incident: 99.12%, Solution: 99.38%)\n",
      "Row 140: Duplicate found (Incident: 94.68%, Solution: 91.51%)\n",
      "Row 150: New unique incident added\n",
      "Row 160: Duplicate found (Incident: 92.38%, Solution: 98.47%)\n",
      "Row 170: New unique incident added\n",
      "Row 180: Duplicate found (Incident: 90.17%, Solution: 97.45%)\n",
      "Row 190: Duplicate found (Incident: 91.78%, Solution: 97.67%)\n",
      "Row 200: Duplicate found (Incident: 92.31%, Solution: 99.50%)\n",
      "Row 210: New unique incident added\n",
      "Row 220: Duplicate found (Incident: 92.41%, Solution: 99.14%)\n",
      "Row 230: Duplicate found (Incident: 96.65%, Solution: 100.00%)\n",
      "Row 240: Duplicate found (Incident: 98.61%, Solution: 97.89%)\n",
      "Row 250: Duplicate found (Incident: 99.67%, Solution: 97.95%)\n",
      "Row 260: Duplicate found (Incident: 100.00%, Solution: 98.33%)\n",
      "Row 270: Duplicate found (Incident: 91.44%, Solution: 90.35%)\n",
      "Row 280: Duplicate found (Incident: 90.24%, Solution: 92.71%)\n",
      "Row 290: New unique incident added\n",
      "Row 300: Duplicate found (Incident: 90.21%, Solution: 96.01%)\n",
      "Row 310: Duplicate found (Incident: 99.57%, Solution: 100.00%)\n",
      "Row 320: New unique incident added\n",
      "\n",
      "============================================================\n",
      "DEDUPLICATION COMPLETE\n",
      "============================================================\n",
      "Total rows processed: 323\n",
      "Duplicates found: 234\n",
      "Unique incidents: 89\n",
      "Reduction: 72.4%\n",
      "============================================================\n",
      "\n",
      "✓ Consolidated incidents saved to: processed-data\\consolidated_incidents.csv\n",
      "\n",
      "Preview of the consolidated data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Incident_Report</th>\n",
       "      <th>Root_Cause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Call ALR-360601 | EDI/API Data Mismatch on HLC...</td>\n",
       "      <td>Time zone drift caused eventTime to serialize ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alert: SMS TCK-265455 | Issue: Spike in DLQ me...</td>\n",
       "      <td>Spike in DLQ messages after routine maintenanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alert: SMS TCK-936729 | Issue: ANSI X12 301 in...</td>\n",
       "      <td>ANSI X12 301 inconsistency for MV SILVER CURRE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: Email ALR-535708 | EDI Parsing Issue ...</td>\n",
       "      <td>EDIFACT CODECO duplicate detected with conflic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alert: SMS TCK-142185 | Issue: Customs flag to...</td>\n",
       "      <td>Customs flag toggled incorrectly for EMCU16695...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Subject: Email ALR-881385 | Vessel MV STELLAR ...</td>\n",
       "      <td>Schedule API intermittently returns 401 for MV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alert: SMS INC-782713 | Issue: Field mapping m...</td>\n",
       "      <td>Field mapping mismatch across systems for cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Call ALR-717476 | Container MAEU0713758 except...</td>\n",
       "      <td>Discrepancy between customer portal and TOS fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alert: SMS INC-125434 | Issue: ETA/ETB mismatc...</td>\n",
       "      <td>ETA/ETB mismatch for MV PACIFIC GLORY (IMO 630...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Subject: Email INC-619806 | API Failure on /bo...</td>\n",
       "      <td>OAuth token rejection spikes on '/bookings/v1'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Incident_Report  \\\n",
       "0  Call ALR-360601 | EDI/API Data Mismatch on HLC...   \n",
       "1  Alert: SMS TCK-265455 | Issue: Spike in DLQ me...   \n",
       "2  Alert: SMS TCK-936729 | Issue: ANSI X12 301 in...   \n",
       "3  Subject: Email ALR-535708 | EDI Parsing Issue ...   \n",
       "4  Alert: SMS TCK-142185 | Issue: Customs flag to...   \n",
       "5  Subject: Email ALR-881385 | Vessel MV STELLAR ...   \n",
       "6  Alert: SMS INC-782713 | Issue: Field mapping m...   \n",
       "7  Call ALR-717476 | Container MAEU0713758 except...   \n",
       "8  Alert: SMS INC-125434 | Issue: ETA/ETB mismatc...   \n",
       "9  Subject: Email INC-619806 | API Failure on /bo...   \n",
       "\n",
       "                                          Root_Cause  \n",
       "0  Time zone drift caused eventTime to serialize ...  \n",
       "1  Spike in DLQ messages after routine maintenanc...  \n",
       "2  ANSI X12 301 inconsistency for MV SILVER CURRE...  \n",
       "3  EDIFACT CODECO duplicate detected with conflic...  \n",
       "4  Customs flag toggled incorrectly for EMCU16695...  \n",
       "5  Schedule API intermittently returns 401 for MV...  \n",
       "6  Field mapping mismatch across systems for cont...  \n",
       "7  Discrepancy between customer portal and TOS fo...  \n",
       "8  ETA/ETB mismatch for MV PACIFIC GLORY (IMO 630...  \n",
       "9  OAuth token rejection spikes on '/bookings/v1'...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the deduplication process with correct column names\n",
    "# The file shows columns: 'Alert / Email' and 'Problem Statements'\n",
    "consolidated_df = deduplicate_incidents(\n",
    "    df,\n",
    "    alert_col='Alert / Email',\n",
    "    problem_col='Problem Statements',\n",
    "    similarity_threshold=0.90\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "consolidated_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"✓ Consolidated incidents saved to: {output_file}\")\n",
    "print(f\"\\nPreview of the consolidated data:\")\n",
    "consolidated_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecffcf5",
   "metadata": {},
   "source": [
    "## Step 6: Verify Results and Test Similarity Function\n",
    "\n",
    "Let's test the similarity function with a few examples to ensure it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "571ef97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Similarity Function\n",
      "============================================================\n",
      "\n",
      "Test 1 - Similar incidents with different IDs:\n",
      "Text 1: Database connection failed for order #12345 at 10:30 AM\n",
      "Text 2: Database connection failed for order #67890 at 2:45 PM\n",
      "Similarity: 100.00% ✓ MATCH\n",
      "\n",
      "Test 2 - Different incidents:\n",
      "Text 1: Database connection failed\n",
      "Text 2: User authentication error occurred\n",
      "Similarity: 61.39% ✗ NO MATCH\n",
      "\n",
      "Test 3 - Nearly identical:\n",
      "Text 1: Server timeout error in production environment\n",
      "Text 2: Server timeout error in production system\n",
      "Similarity: 100.00% ✓ MATCH\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test cases to verify the similarity function\n",
    "print(\"Testing Similarity Function\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Similar incidents with different IDs\n",
    "text1 = \"Database connection failed for order #12345 at 10:30 AM\"\n",
    "text2 = \"Database connection failed for order #67890 at 2:45 PM\"\n",
    "sim1 = calculate_phrase_similarity(text1, text2)\n",
    "print(f\"\\nTest 1 - Similar incidents with different IDs:\")\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 2: {text2}\")\n",
    "print(f\"Similarity: {sim1:.2%} {'✓ MATCH' if sim1 >= 0.90 else '✗ NO MATCH'}\")\n",
    "\n",
    "# Test 2: Different incidents\n",
    "text3 = \"Database connection failed\"\n",
    "text4 = \"User authentication error occurred\"\n",
    "sim2 = calculate_phrase_similarity(text3, text4)\n",
    "print(f\"\\nTest 2 - Different incidents:\")\n",
    "print(f\"Text 1: {text3}\")\n",
    "print(f\"Text 2: {text4}\")\n",
    "print(f\"Similarity: {sim2:.2%} {'✓ MATCH' if sim2 >= 0.90 else '✗ NO MATCH'}\")\n",
    "\n",
    "# Test 3: Nearly identical\n",
    "text5 = \"Server timeout error in production environment\"\n",
    "text6 = \"Server timeout error in production system\"\n",
    "sim3 = calculate_phrase_similarity(text5, text6)\n",
    "print(f\"\\nTest 3 - Nearly identical:\")\n",
    "print(f\"Text 1: {text5}\")\n",
    "print(f\"Text 2: {text6}\")\n",
    "print(f\"Similarity: {sim3:.2%} {'✓ MATCH' if sim3 >= 0.90 else '✗ NO MATCH'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb956f",
   "metadata": {},
   "source": [
    "## Step 7: Final Statistics and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e684370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL SUMMARY\n",
      "============================================================\n",
      "Source file: source-data\\Case Log.xlsx\n",
      "Output file: processed-data\\consolidated_incidents.csv\n",
      "\n",
      "Original incidents: 323\n",
      "Consolidated incidents: 89\n",
      "Duplicates removed: 234\n",
      "Reduction rate: 72.4%\n",
      "============================================================\n",
      "\n",
      "Sample of Consolidated Data:\n",
      "                                                                    Incident_Report                                                                       Root_Cause\n",
      "0   Call ALR-360601 | EDI/API Data Mismatch on HLCU5962669 at PSA Keppel | Issue...  Time zone drift caused eventTime to serialize in UTC+0 for partner Partner-E...\n",
      "1   Alert: SMS TCK-265455 | Issue: Spike in DLQ messages after routine maintenan...  Spike in DLQ messages after routine maintenance; consumer group lag increase...\n",
      "..                                                                              ...                                                                              ...\n",
      "87  SMS INC-108587 | Issue: EDI message REF-COP-0099 received but no acknowledgm...  EDI message REF-COP-0099 was received and processed (status: PARSED), but no...\n",
      "88  Subject: Email ALR-861599 | CMAU0000080 - Customer seeing identical Containe...  Customer on PORTNET is seeing 2 identical containers information.   Please a...\n"
     ]
    }
   ],
   "source": [
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Source file: {source_file}\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "print(f\"\\nOriginal incidents: {len(df)}\")\n",
    "print(f\"Consolidated incidents: {len(consolidated_df)}\")\n",
    "print(f\"Duplicates removed: {len(df) - len(consolidated_df)}\")\n",
    "print(f\"Reduction rate: {((len(df) - len(consolidated_df)) / len(df) * 100):.1f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display sample of consolidated data\n",
    "print(\"\\nSample of Consolidated Data:\")\n",
    "print(consolidated_df.to_string(max_rows=5, max_colwidth=80))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
